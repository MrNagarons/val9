"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML —Ñ–∞–π–ª–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –∫–∞—Ñ–µ
–†–∞–±–æ—Ç–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é
–ù–ï —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –µ—Å—Ç—å
"""

import os
import re
import json
import glob
import concurrent.futures
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
HTML_DIR = "html_files"
METADATA_FILE = os.path.join(HTML_DIR, "metadata.json")
OUTPUT_XLSX = "cafes_astana_table.xlsx"
OUTPUT_CSV = "cafes_astana_table.csv"
MAX_WORKERS = 16


def extract_from_title(title_text):
    """–ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–¥—Ä–µ—Å –∏–∑ title —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    # –§–æ—Ä–º–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–Ω—ã–º:
    # "–ù–∞–∑–≤–∞–Ω–∏–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, —É–ª–∏—Ü–∞, –Ω–æ–º–µ—Ä –¥–æ–º–∞, –ê—Å—Ç–∞–Ω–∞ ‚Äî 2–ì–ò–°"
    # "–ù–∞–∑–≤–∞–Ω–∏–µ, —É–ª–∏—Ü–∞, –Ω–æ–º–µ—Ä –¥–æ–º–∞, –ê—Å—Ç–∞–Ω–∞ ‚Äî 2–ì–ò–°"
    # –ü—Ä–∏–º–µ—Ä: "KaRima, —Ü–µ–Ω—Ç—Ä –ø–ª–æ–≤–∞, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú–∞–Ω–≥–∏–ª–∏–∫ –ï–ª, 54, –ê—Å—Ç–∞–Ω–∞ ‚Äî 2–ì–ò–°"
    # –ü—Ä–∏–º–µ—Ä: "SF, —É–ª–∏—Ü–∞ –ö–∞–π—ã–º –ú—É—Ö–∞–º–µ–¥—Ö–∞–Ω–æ–≤, 19–∞, –ê—Å—Ç–∞–Ω–∞ ‚Äî 2–ì–ò–°"
    if not title_text:
        return None, None

    # –£–±–∏—Ä–∞–µ–º " ‚Äî 2–ì–ò–°" –≤ –∫–æ–Ω—Ü–µ
    title_text = re.sub(r'\s*[‚Äî‚Äì-]\s*2–ì–ò–°$', '', title_text, flags=re.I)

    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ –∑–∞–ø—è—Ç—ã–º
    parts = [p.strip() for p in title_text.split(',')]

    if len(parts) < 2:
        return title_text, None

    name = parts[0]

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—á–∞–ª–∞ –∞–¥—Ä–µ—Å–∞
    address_keywords = [
        '—É–ª–∏—Ü–∞', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '–ø–µ—Ä–µ—É–ª–æ–∫', '–±—É–ª—å–≤–∞—Ä', '—à–æ—Å—Å–µ', '—Ç—É–ø–∏–∫',
        '–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω', '–º–∫—Ä', '—Ä–∞–π–æ–Ω', '–∫–≤–∞—Ä—Ç–∞–ª', '–ø–ª–æ—â–∞–¥—å', '–Ω–∞–±–µ—Ä–µ–∂–Ω–∞—è',
        '–∞–ª–ª–µ—è', '–ø—Ä–æ–µ–∑–¥', '–∂–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å', '–∂–∫', '—É–ª.', '–ø—Ä.', '–ø—Ä-—Ç',
        # –ö–∞–∑–∞—Ö—Å–∫–∏–µ —Å–ª–æ–≤–∞
        '–∫”©—à–µ—Å—ñ', '–¥–∞“£“ì—ã–ª—ã', '—à–∞“ì—ã–Ω –∞—É–¥–∞–Ω—ã'
    ]

    # –ò—â–µ–º –∏–Ω–¥–µ–∫—Å —á–∞—Å—Ç–∏, –≥–¥–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∞–¥—Ä–µ—Å
    address_start_idx = None
    for i, part in enumerate(parts[1:], start=1):
        part_lower = part.lower()
        for kw in address_keywords:
            if kw in part_lower:
                address_start_idx = i
                break
        if address_start_idx is not None:
            break

    if address_start_idx is not None:
        # –ê–¥—Ä–µ—Å - –≤—Å—ë –Ω–∞—á–∏–Ω–∞—è —Å –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        address = ', '.join(parts[address_start_idx:])
        return name, address
    elif len(parts) >= 3:
        # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —á–∞—Å—Ç–∏ –∫–∞–∫ –∞–¥—Ä–µ—Å
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º: –ù–∞–∑–≤–∞–Ω–∏–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –Ω–æ–º–µ—Ä –¥–æ–º–∞, –ì–æ—Ä–æ–¥
        # –∏–ª–∏: –ù–∞–∑–≤–∞–Ω–∏–µ, –Ω–æ–º–µ—Ä –¥–æ–º–∞, –ì–æ—Ä–æ–¥
        address = ', '.join(parts[-2:])
        return name, address
    else:
        return name, parts[-1]


def extract_from_description(desc):
    """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ meta description"""
    if not desc:
        return {}

    result = {}

    # –ß–µ–∫ (—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫)
    check_match = re.search(r'[–ß—á]–µ–∫\s*(\d+(?:\s*\d+)*)\s*(?:—Ç–Ω–≥|—Ç–≥|‚Ç∏)', desc)
    if check_match:
        result['check'] = check_match.group(1).replace(' ', '') + ' —Ç–Ω–≥'

    # –†–µ–π—Ç–∏–Ω–≥
    rating_match = re.search(r'[–û–æ]—Ü–µ–Ω–∫–∞\s*(\d+[.,]\d+)', desc)
    if rating_match:
        result['rating'] = rating_match.group(1).replace(',', '.')

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
    reviews_match = re.search(r'(\d+(?:\s*\d+)*)\s*–æ—Ç–∑—ã–≤', desc)
    if reviews_match:
        result['reviews'] = reviews_match.group(1).replace(' ', '')

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ
    photos_match = re.search(r'(\d+)\s*—Ñ–æ—Ç–æ', desc)
    if photos_match:
        result['photos'] = photos_match.group(1)

    return result


def extract_from_og_description(og_desc):
    """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ og:description (—Ç–∞–º –µ—Å—Ç—å –æ—Ü–µ–Ω–∫–∞)"""
    if not og_desc:
        return {}

    result = {}

    # –§–æ—Ä–º–∞—Ç: "–û—Ü–µ–Ω–∫–∞ 4.5, 180 —Ñ–æ—Ç–æ, 3362 –æ—Ç–∑—ã–≤–∞, —á–µ–∫ 3000 —Ç–Ω–≥."
    rating_match = re.search(r'[–û–æ]—Ü–µ–Ω–∫–∞\s*(\d+[.,]\d+)', og_desc)
    if rating_match:
        result['rating'] = rating_match.group(1).replace(',', '.')

    photos_match = re.search(r'(\d+)\s*—Ñ–æ—Ç–æ', og_desc)
    if photos_match:
        result['photos'] = photos_match.group(1)

    reviews_match = re.search(r'(\d+)\s*–æ—Ç–∑—ã–≤', og_desc)
    if reviews_match:
        result['reviews'] = reviews_match.group(1)

    check_match = re.search(r'[–ß—á]–µ–∫\s*(\d+)\s*(?:—Ç–Ω–≥|—Ç–≥)', og_desc)
    if check_match:
        result['check'] = check_match.group(1) + ' —Ç–Ω–≥'

    return result


def parse_html_file(filepath):
    """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω HTML —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            html = f.read()

        soup = BeautifulSoup(html, 'html.parser')

        # === –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–¥—Ä–µ—Å –∏–∑ title ===
        title_tag = soup.find('title')
        title_text = title_tag.get_text(strip=True) if title_tag else ''
        name, address_from_title = extract_from_title(title_text)

        # === Meta description ===
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        description = desc_tag.get('content', '') if desc_tag else ''
        desc_data = extract_from_description(description)

        # === OG Description (–∑–¥–µ—Å—å —Ä–µ–π—Ç–∏–Ω–≥!) ===
        # –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ BeautifulSoup
        og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})
        og_description = og_desc_tag.get('content', '') if og_desc_tag else ''

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ regex (–±—ã–≤–∞–µ—Ç –Ω–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–≥–∞–º–∏)
        if not og_description:
            og_match = re.search(r'property=["\']og:description["\']\s+content=["\']([^"\']+)["\']', html)
            if og_match:
                og_description = og_match.group(1)

        og_data = extract_from_og_description(og_description)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ (og_data –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ –¥–ª—è —Ä–µ–π—Ç–∏–Ω–≥–∞)
        merged_data = {**desc_data, **og_data}

        # === –¢–µ–ª–µ—Ñ–æ–Ω ===
        phone = None
        phone_links = soup.find_all('a', href=lambda x: x and x.startswith('tel:'))
        if phone_links:
            phone_href = phone_links[0].get('href', '')
            phone = phone_href.replace('tel:', '').strip()

        # === URL –∏–∑ canonical link ===
        url_2gis = None
        canonical = soup.find('link', rel='canonical')
        if canonical:
            url_2gis = canonical.get('href', '')

        # === –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ description ===
        categories = []
        if description:
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ "–∫–∞–∫ –¥–æ–µ—Ö–∞—Ç—å." –∏ –¥–æ —Ç–æ—á–∫–∏
            cat_match = re.search(r'–∫–∞–∫ –¥–æ–µ—Ö–∞—Ç—å\.\s*([^.]+)', description, re.I)
            if cat_match:
                cat_text = cat_match.group(1)
                categories = [c.strip() for c in cat_text.split(',') if c.strip()][:5]

        return {
            'name': name or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
            'address': address_from_title or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
            'phone': phone or '–ù–µ —É–∫–∞–∑–∞–Ω–æ',
            'rating': merged_data.get('rating', '–ù–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–∞'),
            'reviews': merged_data.get('reviews', '0'),
            'photos': merged_data.get('photos', '0'),
            'avg_check': merged_data.get('check', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
            'categories': ', '.join(categories) if categories else '–ö–∞—Ñ–µ',
            'url_2gis': url_2gis or '',
            'status': 'success'
        }

    except Exception as e:
        return {
            'name': '–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞',
            'address': '',
            'phone': '',
            'rating': '',
            'reviews': '',
            'photos': '',
            'avg_check': '',
            'categories': '',
            'url_2gis': '',
            'status': 'error',
            'error': str(e)[:100]
        }


def parse_with_id(args):
    """–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    idx, filepath = args
    result = parse_html_file(filepath)
    result['id'] = idx
    result['filename'] = os.path.basename(filepath)
    return result


def main():
    print(f"{'=' * 60}")
    print("–ü–ê–†–°–ò–ù–ì HTML –§–ê–ô–õ–û–í –ò –°–û–ó–î–ê–ù–ò–ï –¢–ê–ë–õ–ò–¶–´")
    print(f"{'=' * 60}\n")

    # –°–ø–æ—Å–æ–± 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å metadata.json –µ—Å–ª–∏ –µ—Å—Ç—å
    if os.path.exists(METADATA_FILE):
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ {METADATA_FILE}...")
        with open(METADATA_FILE, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        files_to_parse = []
        for item in metadata:
            filepath = os.path.join(HTML_DIR, item['filename'])
            if os.path.exists(filepath):
                files_to_parse.append((item['id'], filepath))

        print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(files_to_parse)} —Ñ–∞–π–ª–æ–≤ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º")
    else:
        # –°–ø–æ—Å–æ–± 2: –ù–∞–π—Ç–∏ –≤—Å–µ HTML —Ñ–∞–π–ª—ã
        print(f"üìÇ –ò—â—É HTML —Ñ–∞–π–ª—ã –≤ {HTML_DIR}...")
        html_files = glob.glob(os.path.join(HTML_DIR, "cafe_*.html"))

        files_to_parse = []
        for filepath in html_files:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            match = re.search(r'cafe_(\d+)\.html', filepath)
            if match:
                idx = int(match.group(1))
                files_to_parse.append((idx, filepath))

        files_to_parse.sort(key=lambda x: x[0])
        print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(files_to_parse)} HTML —Ñ–∞–π–ª–æ–≤")

    if not files_to_parse:
        print("‚ùå HTML —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ download_html.py –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü")
        return

    print(f"\nüîÑ –ü–∞—Ä—Å–∏–Ω–≥ {len(files_to_parse)} —Ñ–∞–π–ª–æ–≤ ({MAX_WORKERS} –ø–æ—Ç–æ–∫–æ–≤)...\n")

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    results = []
    success_count = 0
    error_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(parse_with_id, item): item[0] for item in files_to_parse}

        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            results.append(result)

            if result.get('status') == 'success':
                success_count += 1
                if success_count % 100 == 0:
                    print(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count} / {len(files_to_parse)}")
            else:
                error_count += 1
                if error_count <= 5:
                    print(f"‚úó –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {result.get('filename', '?')}: {result.get('error', 'unknown')[:50]}")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ID (–ù–ï —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã!)
    results.sort(key=lambda x: x.get('id', 0))

    # –§–æ—Ä–º–∏—Ä—É–µ–º DataFrame
    df_data = []
    for r in results:
        if r.get('status') == 'success':
            df_data.append({
                '‚Ññ': r.get('id', 0),
                '–ù–∞–∑–≤–∞–Ω–∏–µ': r.get('name', ''),
                '–ê–¥—Ä–µ—Å': r.get('address', ''),
                '–¢–µ–ª–µ—Ñ–æ–Ω': r.get('phone', ''),
                '–†–µ–π—Ç–∏–Ω–≥': r.get('rating', ''),
                '–û—Ç–∑—ã–≤–æ–≤': r.get('reviews', ''),
                '–§–æ—Ç–æ': r.get('photos', ''),
                '–°—Ä–µ–¥–Ω–∏–π —á–µ–∫': r.get('avg_check', ''),
                '–ö–∞—Ç–µ–≥–æ—Ä–∏–∏': r.get('categories', ''),
                'URL 2GIS': r.get('url_2gis', ''),
                'HTML —Ñ–∞–π–ª': r.get('filename', '')
            })

    if not df_data:
        print("\n‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")
        return

    df = pd.DataFrame(df_data)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel –∏ CSV
    df.to_excel(OUTPUT_XLSX, index=False, engine='openpyxl')
    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')

    print(f"\n{'=' * 60}")
    print("‚úÖ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
    print(f"{'=' * 60}")
    print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}")
    print(f"‚úó –û—à–∏–±–æ–∫: {error_count}")
    print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ: {len(df)}")
    print(f"üìÅ Excel: {OUTPUT_XLSX}")
    print(f"üìÅ CSV: {OUTPUT_CSV}")
    print(f"{'=' * 60}\n")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  - –° —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏: {len(df[df['–¢–µ–ª–µ—Ñ–æ–Ω'] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ'])}")
    print(f"  - –° —Ä–µ–π—Ç–∏–Ω–≥–æ–º: {len(df[df['–†–µ–π—Ç–∏–Ω–≥'] != '–ù–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–∞'])}")
    print(f"  - –°–æ —Å—Ä–µ–¥–Ω–∏–º —á–µ–∫–æ–º: {len(df[df['–°—Ä–µ–¥–Ω–∏–π —á–µ–∫'] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ'])}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏
    print("\nüìã –ü–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π:")
    print(df[['‚Ññ', '–ù–∞–∑–≤–∞–Ω–∏–µ', '–ê–¥—Ä–µ—Å', '–¢–µ–ª–µ—Ñ–æ–Ω', '–†–µ–π—Ç–∏–Ω–≥']].head().to_string(index=False))


if __name__ == "__main__":
    main()






