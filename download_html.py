"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ñ–µ –ø–æ —Å–ø–∏—Å–∫—É URL –∏–∑ all_urls.txt
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
"""

import os
import re
import json
import concurrent.futures
import requests
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
HTML_DIR = "html_files"
URLS_FILE = os.path.join(HTML_DIR, "all_urls.txt")
METADATA_FILE = os.path.join(HTML_DIR, "metadata.json")
MAX_WORKERS = 16  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –±–æ–ª—å—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏)
TIMEOUT = 15  # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
}


def parse_urls_file(filepath):
    """–ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª —Å URL –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'N. URL'"""
    urls = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # –§–æ—Ä–º–∞—Ç: "1. https://..."
            match = re.match(r'^(\d+)\.\s+(.+)$', line)
            if match:
                idx = int(match.group(1))
                url = match.group(2).strip()
                urls.append((idx, url))
    return urls


def download_and_save(args):
    """–°–∫–∞—á–∞—Ç—å HTML –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª"""
    idx, url = args
    filename = f"cafe_{idx:04d}.html"
    filepath = os.path.join(HTML_DIR, filename)

    try:
        # –°–æ–∑–¥–∞—ë–º —Å–µ—Å—Å–∏—é –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        response = requests.get(url, timeout=TIMEOUT, headers=HEADERS)
        response.raise_for_status()

        html_content = response.text

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)

        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ <h1>
        quick_name = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', html_content, re.I | re.S)
        if h1_match:
            quick_name = re.sub(r'<[^>]+>', '', h1_match.group(1)).strip()

        return {
            "id": idx,
            "filename": filename,
            "url": url,
            "quick_name": quick_name,
            "status": "success"
        }

    except requests.exceptions.Timeout:
        return {"id": idx, "url": url, "status": "error", "error": "timeout"}
    except requests.exceptions.RequestException as e:
        return {"id": idx, "url": url, "status": "error", "error": str(e)[:100]}
    except Exception as e:
        return {"id": idx, "url": url, "status": "error", "error": str(e)[:100]}


def main():
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å URL
    if not os.path.exists(URLS_FILE):
        print(f"‚ùå –§–∞–π–ª {URLS_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ main_collect.py –¥–ª—è —Å–±–æ—Ä–∞ URL")
        return

    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(HTML_DIR, exist_ok=True)

    # –ü–∞—Ä—Å–∏–º URL
    print(f"üìÇ –ß–∏—Ç–∞—é URL –∏–∑ {URLS_FILE}...")
    urls = parse_urls_file(URLS_FILE)
    print(f"‚úì –ù–∞–π–¥–µ–Ω–æ URL: {len(urls)}")

    if not urls:
        print("‚ùå URL –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ñ–∞–π–ª–µ!")
        return

    print(f"\n{'=' * 60}")
    print(f"–ó–ê–ì–†–£–ó–ö–ê HTML ({MAX_WORKERS} –ø–æ—Ç–æ–∫–æ–≤)")
    print(f"{'=' * 60}\n")

    metadata = []
    success_count = 0
    error_count = 0

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # –°–æ–∑–¥–∞—ë–º –≤—Å–µ –∑–∞–¥–∞—á–∏
        future_to_idx = {executor.submit(download_and_save, item): item[0] for item in urls}

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        for future in concurrent.futures.as_completed(future_to_idx):
            result = future.result()

            if result["status"] == "success":
                success_count += 1
                metadata.append({
                    "id": result["id"],
                    "filename": result["filename"],
                    "url": result["url"],
                    "quick_name": result["quick_name"]
                })

                # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Ñ–∞–π–ª–æ–≤
                if success_count % 50 == 0:
                    print(f"‚úì –°–∫–∞—á–∞–Ω–æ: {success_count} / {len(urls)}")
            else:
                error_count += 1
                if error_count <= 10:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –æ—à–∏–±–æ–∫
                    print(f"‚úó –û—à–∏–±–∫–∞ #{result['id']}: {result.get('error', 'unknown')[:50]}")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ id
    metadata.sort(key=lambda x: x["id"])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    with open(METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"\n{'=' * 60}")
    print("–ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"{'=' * 60}")
    print(f"‚úì –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {success_count}")
    print(f"‚úó –û—à–∏–±–æ–∫: {error_count}")
    print(f"üìÅ HTML —Ñ–∞–π–ª—ã: {HTML_DIR}/")
    print(f"üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {METADATA_FILE}")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()

